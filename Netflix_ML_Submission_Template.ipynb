{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shajad121/Netflix-capston-project/blob/main/Netflix_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Netflix**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is about the Credit card default prediction analysis. When we start this project, it is very difficult to understand and look very complex. In first step we upload the data and then we start to find the duplicate values and nulls values and missing values etc. in this dataset we found the some values are missing in Cast and Directors columns so we decide to remove that colums also in release year there are sum values also missing we fill that values with mean and we also fill the country columns null values with mode in. After completion of 1st step in 2nd step we plot the data in form of graph, pie plot and bar plot and line plot and also box plot, and in box plot we find some outlier so we remove the outliers. And we also find some hidden information by plot the graph. In this dataset there are two types of contents 30.86% is Tv shows and 69.14% is movies includes. We have reached a conclusion from our analysis from the content added over years that Netflix is focusing movies and TV shows (From 2016 data we get to know that Movies is increased by 80% and TV shows is increased by 73% compare). From the dataset insights we can conclude that, most number of TV Shows released in 2017 and for Movies it is 2020. On Netflix USA has the largest number of contents. And most of the countries preferred to produce movies more than TV shows. And then India comers on number 2 hos produce the largest movies and the tv show. Most of the movies are belonging to 3 categories. TOP 3 content categories are, International movies, dramas, comedies. there are 35.1% movies and tv shows are from the United State. 12.7% movies and tv shows are from the India. 5.5% tv shows and movies from the United Kingdom.3.1% tv shows and movies are from the Japan. 2.5% movies and shows are from the South Korea. We also devide the data into month year and the day of month and add that columns into the data frame and we also create some Dataframe in our data set.  In text analysis (NLP) I used stop words, removed punctuations, stemming & TF-IDF vectorizer and other functions of NLP. Applied different clustering models like K-means, hierarchical, Agglomerative clustering, DBSCAN on data we got the best cluster arrangements. By applying different clustering algorithms to our dataset . we get the optimal number of cluster is equal to 3 .So we decide to take the as no of cluster in this data because it is best no in our conclueion.   "
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Shajad121/Netflix-capston-project.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "import sklearn\n",
        "from collections import Counter\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn import metrics  \n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Display preference\n",
        "pd.set_option('Display.max_columns', 500)\n",
        "pd.set_option('Display.max_rows', 10000)\n",
        "#for nlp\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import datetime as dt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.datasets import make_blobs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_C-gmFlEjK_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/netflix /file.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "msno.bar(df,labels=True,figsize=(10,5))"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 7787 rows and 12 columns present in this data set. each columns represent the different information about the data. there are no duplicate values present in this data set. there are 11 pbject type and q1 is int datatype. and there are some null values present in this data set. approx 30% director name are missing in this data.718 in cast name 510 in country and 10 in date added rows is null in this data set"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id --> Unique id for every movie/tv show\n",
        "\n",
        "type --> identifier -A movie or tv show\n",
        "\n",
        "title --> Title of the movie and tv show\n",
        "\n",
        "director --> director of the show\n",
        "\n",
        "cast --> Actors involved\n",
        "\n",
        "country --> Country of production\n",
        "\n",
        "date_added --> date it was added at Netflix\n",
        "\n",
        "release_year --> Actual release year of the show\n",
        "\n",
        "rating --> TV raring of the show\n",
        "\n",
        "duration --> Total duration in minutes or no. of the seasons\n",
        "\n",
        "listed_in --> Genre\n",
        "\n",
        "description -->The summary description"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df :\n",
        "  print(f'Unique values for {i}: {df[i].unique()}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# import the date time\n",
        "import datetime"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a copy of the data\n",
        "df2 =df.copy()"
      ],
      "metadata": {
        "id": "W3xKkwP0X-Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the columns \n",
        "df2.drop(['director','cast'],axis=1, inplace=True)\n",
        "df2.dropna(subset = [ 'date_added' ], inplace = True)"
      ],
      "metadata": {
        "id": "c5wxwta4ZMF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the yearly movies and sows\n",
        "yearly_movies=df2[df2.type =='TV Show']['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "yearly_shows=df2[df2.type =='Movie']['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "total_content=df2['release_year'].value_counts().sort_index(ascending=False).head(15)"
      ],
      "metadata": {
        "id": "k2qiuzpbZMCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "release_year_Q1 = df2.release_year.quantile(0.25)\n",
        "release_year_Q3 = df2.release_year.quantile(0.75)\n",
        "release_year_IQR = release_year_Q3 - release_year_Q1\n",
        "print(f'release_year_Q1 = {release_year_Q1}')\n",
        "print(f'release_year_Q3 = {release_year_Q3}')\n",
        "print(f'release_year_IQR = {release_year_IQR}')"
      ],
      "metadata": {
        "id": "3HIsUDWDZL9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset index\n",
        "df2.country.value_counts().rename_axis('Country').reset_index(name='counts').T"
      ],
      "metadata": {
        "id": "Gc8VpIzhdKjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create country as list\n",
        "country_list=[]\n",
        "tv_show=[]\n",
        "movies=[]\n",
        "for i in range(0,len(df2)):\n",
        "  if isinstance(df2['country'].iloc[i] , str):\n",
        "    split=df2['country'].iloc[i].split(',')\n",
        "    for k in split:\n",
        "      country_list.append(k.strip())\n",
        "      if df2['type'].iloc[i]=='TV Show':\n",
        "        tv_show.append(k.strip())\n",
        "      if df2['type'].iloc[i]== 'Movie':\n",
        "        movies.append(k.strip())\n",
        "production_country=list(set([(i,country_list.count(i),tv_show.count(i),movies.count(i)) for i in country_list]))"
      ],
      "metadata": {
        "id": "e_AHjO8OhmAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new data formate country\n",
        "country_df= pd.DataFrame(production_country,columns=['country','Productions','TV-Shows','Movies']) \n",
        "country_df=country_df.sort_values('Productions',ascending=False)\n",
        "country_df=country_df.reset_index()\n",
        "country_df=country_df.drop('index',axis=1)\n",
        "     \n",
        "\n",
        "#Top 5 countries\n",
        "top_countries=country_df.head()\n",
        "top_countries"
      ],
      "metadata": {
        "id": "238IW9pSdKfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['date_added_month'] = df2['date_added'].apply(lambda x: x.split(\" \")[0])\n",
        "df2[['date_added' , 'date_added_month']].head()\n",
        "     "
      ],
      "metadata": {
        "id": "JU6hx303dKdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_df=df2['date_added_month'].value_counts().reset_index()\n",
        "month_df.rename(columns={'index': 'Month_Name'}, inplace=True)\n",
        "month_df.rename(columns={'month': 'Count'}, inplace=True)\n",
        "ab = month_df.loc[0:11]\n",
        "ab\n",
        "     "
      ],
      "metadata": {
        "id": "2Z2WrrPLdKaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.description.iloc[0]\n",
        "    \n",
        "First_des = df2.description.iloc[0]\n",
        "First_des\n",
        "     "
      ],
      "metadata": {
        "id": "dEKTTbS2Zyni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "df['type'].value_counts().plot.pie(autopct = '%1.1f%%')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the types of data"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this data there are 69% are movies and 30.9% are tv shows present in this data\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "df['country'].value_counts().plot.pie(autopct = '%1.1f%%')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to check the top 5 country in production of movies and shows"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "according to this plot\n",
        "\n",
        "there are 35.1% movies and tv shows are from the United State.\n",
        "\n",
        "12.7% movies and tv shows are from the India.\n",
        "\n",
        "5.5% tv shows and movies from the United Kingdom.\n",
        "\n",
        "3.1% tv shows and movies are from the Japan.\n",
        "\n",
        "2.5% movies and shows are from the South Korea"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the date time\n",
        "import datetime"
      ],
      "metadata": {
        "id": "rr1eJGYHf1Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tha date into Datetimeobject\n",
        "df['date_added'] = pd.to_datetime(df['date_added'])"
      ],
      "metadata": {
        "id": "NtoMbQmbfypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st we split the date added into year month and the day\n",
        "df['day_added'] = df['date_added'].dt.day\n",
        "df['week_added'] = df['date_added'].dt.week\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df['year_added']  = df['date_added'].dt.year"
      ],
      "metadata": {
        "id": "CfGb2HNgfuL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.set(font_scale=1.4)\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "(df['year_added'].value_counts()).plot(kind = 'bar',figsize=(12, 6), color='blue')\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.legend()\n",
        "plt.title(\"Production Growth Yearly\", y=1.02, fontsize=22);"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to find to no of movieas and show produce in the year"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this lot we show that more then 2000 movies produce in 2019\n",
        "\n",
        "2000 movieas and shows produce in 2020\n",
        "\n",
        "1600 movies and shows produce in 2018 aproximatly.\n",
        "\n",
        "1200 movies and shows are produce in 2017\n",
        "\n",
        "less them 500 shows and movies produce per year before 2017"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize = (12 ,8 ))\n",
        "(df2['date_added_month'].value_counts()).plot(kind = 'bar')\n",
        "plt.title(\"Month wise content barplot\" , fontsize = 19)\n",
        "plt.xlabel(\"Month\" , fontsize = 15)\n",
        "plt.ylabel(\"Count\" , fontsize = 15)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the count according to the month"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this plot we show that in december more then 800 movies and shows aded on net flix and then november between 750-800 movies and shows are added on neflix and then january 750 shows and movies are added on january and february is the least movies and shows added month."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize = (12 ,8 ))\n",
        "(df['day_added'].value_counts()).plot(kind = 'bar')\n",
        "plt.title(\"Month wise content barplot\" , fontsize = 19)\n",
        "plt.xlabel(\"day\" , fontsize = 15)\n",
        "plt.ylabel(\"Count\" , fontsize = 15)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the movies and shows according the days"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "according to this plot 1st days of every month to most movies and shows are added on the netflix and then the 15th day of month the movies and the shows added on the netflix between the 650to 700. and then the last day of the month is most released day of netfflix it is between the 200 to 250."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#finding the outliers in release year\n",
        "plt.figure(figsize=(20,7))\n",
        "sns.boxplot(x=df['release_year'])    "
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the outliers"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this plot we see that after the 2012 to 2021. is founded as outliers and we delet this outliers."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "tv_show = df[df['type']== 'TV Show' ]\n",
        "#Pointplot on top tv show ratings \n",
        "plt.figure(figsize=(12,6))\n",
        "tv_ratings = tv_show.groupby(['rating'])['show_id'].count().reset_index(name = 'count').sort_values(by = 'count', ascending = False)\n",
        "fig_dims = (7,4)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('Top TV Show Ratings Based On Rating System')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the tv_show according to the rating"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA (For Mature Audiences).\n",
        "\n",
        "TV-14 ( May be unsuitable for children under 14 ).\n",
        "\n",
        "TV-PG ( Parental Guidance Suggested ).\n",
        "\n",
        "NR ( Not Rated ) "
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "movies = df[df['type'] == 'Movie' ]\n",
        "#Pointplot on top tv show ratings \n",
        "plt.figure(figsize=(12,6))\n",
        "tv_ratings = movies.groupby(['rating'])['show_id'].count().reset_index(name = 'count').sort_values(by = 'count', ascending = False)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('Top Movie Ratings Based On Rating System',size='20')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the movies according to the rating system"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA (For Mature Audiences)1900 movies come under this rating.\n",
        "\n",
        "TV-14 ( May be unsuitable for children under 14 ) 1250 movies come under this rating.\n",
        "\n",
        "TV-PG ( Parental Guidance Suggested ) 500 movies come under this rating.\n",
        "\n",
        "NR ( Not Rated ) 150 movies come under this rating."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.plot\n",
        "df.duration.value_counts().plot(kind='line')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to finde the duration outliers"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nan"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to finde out the relation"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In thid corilation heatmap we finde that the 86% corilation between the week_added and the month_added."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df,hue='rating')"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create new columns tv show\n",
        "tv_show = df2[df2['type']== 'TV Show' ]\n",
        "tv_show.head(2)"
      ],
      "metadata": {
        "id": "MCmovkBMtYG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new columns movies\n",
        "movies = df2[df2['type'] == 'Movie' ]\n",
        "movies.head(2)"
      ],
      "metadata": {
        "id": "z5Cb-mtRtPiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# fill the null values in year with mean\n",
        "df2[\"release_year\"] = np.where(df2[\"release_year\"] <2009, df2.release_year.mean(),df2['release_year'])"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fil the country with mode\n",
        "# count mode\n",
        "df2.country.mode()[0]\n",
        "#fill the null value\n",
        "df2.country[df2.country.isna()] = df2.country.mode()[0]\n",
        " #set    \n",
        "df2.country[df2.country.isna()]\n",
        "     \n",
        "df.country.isna().sum()"
      ],
      "metadata": {
        "id": "3f3aOFwng72n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We delete the directors column and casts columns"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handling outlier in release year\n",
        "ry_outliers = df2[(df2.release_year < (release_year_Q1 - 1.5 * release_year_IQR)) |\n",
        "              ( df2.release_year > (release_year_Q3 + 1.5 * release_year_IQR)) ]                                            \n",
        "ry_outliers.head(3)"
      ],
      "metadata": {
        "id": "lh8pqCHcfHnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "# 15 percentile value is 2009\n",
        "df[\"release_year\"] = np.where(df[\"release_year\"] <2009, df.release_year.mean(),df['release_year'])\n",
        "print(f\"Datatype of release_year = \",type(df.release_year.iloc[0]))\n",
        "# change the dtype \n",
        "df.release_year = df.release_year.astype(\"int64\")\n",
        "print(f\"Datatype of release_year = \",type(df.release_year.iloc[0]))\n",
        "                                "
      ],
      "metadata": {
        "id": "nvMrbpWOKjjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# create the function (remove_punctuation)\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space, \n",
        "    # which in effect deletes the punctuation marks \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    \n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the punctuation\n",
        "df2['description'] = df2['description'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "EPHLeMttx-YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***removing punctuation from listed_in***"
      ],
      "metadata": {
        "id": "AiNcFrW1FQrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the punctuations from listed_in\n",
        "df2['listed_in'] = df2['listed_in'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "4vrD4dnsFacz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# extracting the stopwords from nltk library\n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "for i in sw:\n",
        "  print(i , end=',  ')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creat the function to remove the stopwords\n",
        "def remove_stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text1 = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text1)\n",
        "     \n",
        "# Remove White spacesand sto_words\n",
        "df2['description'] = df2['description'].apply( remove_stopwords )\n"
      ],
      "metadata": {
        "id": "RKpo0FKGywOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***removing stop words and white space from the listed_in***"
      ],
      "metadata": {
        "id": "8PYVYAyEFvUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove stopwords from listed_in\n",
        "df2['listed_in'] = df2['listed_in'].apply( remove_stopwords )\n",
        "     "
      ],
      "metadata": {
        "id": "iviO0GGKF_X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Create the count vectorizer object\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cntv = CountVectorizer()\n",
        "\n",
        "# Fit the count vectorizer using the text data\n",
        "cntv.fit(df2['description'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dic = cntv.vocabulary_.items()\n",
        "#  creat Lists to store the vocab and counts\n",
        "vocab = []\n",
        "count_of_vocab = []\n",
        "# append the vocab and count to key and value\n",
        "for key, value in dic:\n",
        "    vocab.append(key)\n",
        "    count_of_vocab.append(value)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame vocab_before_stemming\n",
        "\n",
        "# Store the count in panadas dataframe with vocab as index\n",
        "vocab_before_stemming = pd.DataFrame({'Word':vocab,'count':count_of_vocab})\n",
        "# Sort the dataframe\n",
        "vocab_before_stemming = vocab_before_stemming.sort_values('count' ,ascending=False)"
      ],
      "metadata": {
        "id": "ZOTyZeT40jWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before_stemming.head(20).T"
      ],
      "metadata": {
        "id": "pU6QfBmn8T07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the top 20 most usable vocab\n",
        "top20_most_used_vocab = vocab_before_stemming.head(20)\n",
        "top20_most_used_words = top20_most_used_vocab.Word.values \n",
        "# count the no of most used words\n",
        "top20_most_used_words_count = top20_most_used_vocab['count'].values\n",
        "print(f'top used words{top20_most_used_words}')\n",
        "print(f'counts of top used words{top20_most_used_words_count}')\n",
        "          "
      ],
      "metadata": {
        "id": "Hb89bp4y0jTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the graph\n",
        "plt.figure( figsize = ( 12,6 ))\n",
        "plt.xlim(19550, 19610)\n",
        "plt.barh(top20_most_used_words , top20_most_used_words_count )\n",
        "     "
      ],
      "metadata": {
        "id": "lt3YfSzf24Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***vectoization in listed in***"
      ],
      "metadata": {
        "id": "NwjLyFvPAp8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count vectorization in listed_in\n",
        "# Create a count vectorizer object\n",
        "ctv = CountVectorizer()\n",
        "# fit the count vectorizer using the text data\n",
        "ctv.fit(df2['listed_in'])\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dic = ctv.vocabulary_.items()\n",
        "#  creat Lists to store the vocab and counts\n",
        "vocab = [ ]\n",
        "count_of_vocab = []\n",
        "for key , value in dic:\n",
        "  vocab.append( key )\n",
        "  count_of_vocab.append( value )"
      ],
      "metadata": {
        "id": "Xvwfkam3_yND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame vocab_before_stemming\n",
        "listed_in_vocab_before_stem = pd.DataFrame({\"Word\": vocab , \"count\" :count_of_vocab})\n",
        "\n",
        "listed_in_vocab_before_stem = listed_in_vocab_before_stem.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "Il-lJotL_yJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the value\n",
        "top_20_most_used_vocab_listed_in = listed_in_vocab_before_stem.head(15)\n",
        "     \n",
        "#top most words in listed_in\n",
        "top_20_most_used_words_listed_in = top_20_most_used_vocab_listed_in.Word.values \n",
        "print(f'top most used words are {top_20_most_used_words_listed_in}')\n",
        "# top used words count\n",
        "top_20_most_used_words_in_listed_in_count = top_20_most_used_vocab_listed_in['count'].values \n",
        "print(f'top most used words count {top_20_most_used_words_in_listed_in_count}')\n",
        "     "
      ],
      "metadata": {
        "id": "vCoNYAWr_yGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the graph\n",
        "plt.figure( figsize = ( 12,6 ))\n",
        "plt.xlim(25, 45 )\n",
        "plt.barh(top_20_most_used_words_listed_in , top_20_most_used_words_in_listed_in_count )\n",
        "     "
      ],
      "metadata": {
        "id": "NRXF4pth_yAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use count vector technique.it makes easy for ttextdata to be used directoly in module as text classification "
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "def convert_seasons_to_min(value):\n",
        "  \"\"\"\n",
        "  This function will calculate no of total mins as per season no.\n",
        "  Here our assumptions are\n",
        "    1. on average 5 episodes are there in a season.\n",
        "    2. each episode avg time is 55 mins.\n",
        "  \"\"\"\n",
        "  no_of_avg_episode = 5\n",
        "  if \"Seasons\" in value:\n",
        "    #containing more than 1 seasons\n",
        "    value = value.replace(\"Seasons\",'')\n",
        "    value = value.replace(\" \",\"\")\n",
        "    total_seasons = int(value)\n",
        "    each_season_mins = ( no_of_avg_episode * 55 )\n",
        "    total_mins = (total_seasons * each_season_mins)\n",
        "    return total_mins\n",
        "\n",
        "  elif \"Season\" in value:\n",
        "    # containing only 1 season\n",
        "    value = value.replace(\"Season\",'')\n",
        "    value = value.replace(\" \",\"\")\n",
        "    total_mins = (no_of_avg_episode * 55)\n",
        "    return total_mins\n",
        "     \n",
        "\n",
        "#Checking the function\n",
        "convert_seasons_to_min(\"4 Seasons\")\n",
        "     "
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_the_duration_in_minutes():\n",
        "  \"\"\"\n",
        "  This function will convert all the duration \n",
        "  whether it's in minutes or season format to minute\n",
        "  \"\"\"\n",
        "  # replaced all the min with null string\n",
        "  df['duration'] = df.duration.str.replace(\" min\" , \"\")\n",
        "  # this time_list will contain all the value\n",
        "  time_list =[]\n",
        "  for time in df.duration.values:\n",
        "    if \"Season\" in time:\n",
        "      #time is containing Season\n",
        "      # calling convert_seasons_to_min function to convert \n",
        "      # season to total min \n",
        "      time = convert_seasons_to_min(time)\n",
        "    else:\n",
        "      #replacing single space with \"\"\n",
        "      time = time.replace(\" \",\"\")\n",
        "    #appending time (it's not containing words like min or seasons)\n",
        "    time_list.append(time)\n",
        "\n",
        "  #converting all the time into integer format\n",
        "  time_list = [ int(Time) for Time in time_list]\n",
        "\n",
        "  #Assigning time_list to df.duration\n",
        "  df.duration = time_list \n",
        "     \n",
        "\n",
        "df2.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "qhu7yzymiH5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "bdrmR-BbiMz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = \", \".join(df2['listed_in']).split(\", \")\n",
        "categories[:5]"
      ],
      "metadata": {
        "id": "7l9vkw65iMwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_wise_count = {}\n",
        "for category in set(categories):\n",
        "  category_wise_count[category] = categories.count(category)\n",
        "     \n",
        "category_wise_count"
      ],
      "metadata": {
        "id": "HkVZmhdEiMt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort the category wise count\n",
        "sorted_category_wise_count = sorted(category_wise_count.items(), key=lambda x: x[1])\n",
        "# top 4 category\n",
        "sorted_category_wise_count[:4]\n",
        "     "
      ],
      "metadata": {
        "id": "LSab6JaGicvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top 10 most occurred category\n",
        "top_10_most_occurred_categories = sorted_category_wise_count[-10:]\n",
        "     \n",
        "top_10_most_occurred_categories"
      ],
      "metadata": {
        "id": "6rr61yvTicsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_category_name = []\n",
        "top_10_most_occurred_category_count = []\n",
        "for tup in top_10_most_occurred_categories:\n",
        "  top_10_most_occurred_category_name.append(tup[0])\n",
        "  top_10_most_occurred_category_count.append(tup[1])\n",
        "     \n",
        "\n",
        "top_10_most_occurred_category_name"
      ],
      "metadata": {
        "id": "z5eFZBI5icqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "(df2.listed_in.iloc[0]).split(\",\")"
      ],
      "metadata": {
        "id": "Q3st3isgicny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length \n",
        "len((df2.listed_in.iloc[0]).split(\",\"))"
      ],
      "metadata": {
        "id": "kdiKzrH3jO3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no. of category\n",
        "no_of_category = []\n",
        "for categories in df2.listed_in.values:\n",
        "  len_categories = len(categories.split(\",\"))\n",
        "  no_of_category.append(len_categories)\n",
        "     \n",
        "df2['no_of_category'] = no_of_category\n",
        "     \n",
        "df2[['listed_in' , 'no_of_category']].head()\n",
        "     "
      ],
      "metadata": {
        "id": "7caxYP5_jO01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Length(listed-in)'] = df2['listed_in'].apply(lambda x: len(x))\n",
        "df2.head(3)"
      ],
      "metadata": {
        "id": "Og83NCi0jOy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Length(description)'] = df2['description'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "2CaTn276jOwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "Ka7bXgU_L0AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "stdscaler = preprocessing.StandardScaler()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_features = df2[['no_of_category' ,'Length(description)','Length(listed-in)']]\n",
        "stdscaler = preprocessing.StandardScaler()"
      ],
      "metadata": {
        "id": "T5WHUdxfIU5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdc=stdscaler.fit_transform(new_features)\n",
        "X=sdc\n"
      ],
      "metadata": {
        "id": "hlSzmV_5Hrdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# in unsupervised machine learning data spliting is not requared"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we dont split the data into train and test split because in unsupervised machine learning split is not requared"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "no we dont think that the data is inbalanced"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nan"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Hierarchical Clustering  (1.1 Dendrogram )"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict and plot the graph\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'single'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "vS_TxnjijYgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muaEFWKX8WlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Hierarchical Clustering  (1.2 AgglomerativeClustering )"
      ],
      "metadata": {
        "id": "qJa_uf528D0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# fit the algorithm\n",
        "agc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "# predict algorithm\n",
        "y_agc = agc.fit_predict(X)"
      ],
      "metadata": {
        "id": "OkxPNFy18dJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the graph\n",
        "# Visualizing the clusters (three dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_agc == 0, 0], X[y_agc == 0, 1], s = 100, c = 'red', label = '1')\n",
        "plt.scatter(X[y_agc == 1, 0], X[y_agc == 1, 1], s = 100, c = 'blue', label = '2')\n",
        "# plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = '3')\n",
        "\n",
        "plt.title('Clusters of content')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AQ9JrBp984eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3  K-Means Clstring ( 2.1 Elbow method )"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "    \n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2gnNeQX5ejz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this graph 3 no of cluster is the best option"
      ],
      "metadata": {
        "id": "P2Ed2hhi5qSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the no. of cluster\n",
        "kmeans = KMeans(n_clusters = 3 )\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)\n",
        "     "
      ],
      "metadata": {
        "id": "21RDFP795yfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the graph\n",
        "\n",
        "plt.figure(figsize=(10 , 6))\n",
        "plt.title('description and listed_in')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='spring')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n"
      ],
      "metadata": {
        "id": "SCOknHbg5ybr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ocfSkDuH5yZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4  K-Means Clstring ( 2.2 DBSCAN )"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)\n",
        "# plot the graph\n",
        "plt.figure(figsize=( 8 , 5 ))\n",
        "plt.scatter(X[:,1], X[:,2], c=y_pred)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is about the Credit card default prediction analysis. When we start this project, it is very difficult to understand and look very complex. In first step we upload the data and then we start to find the duplicate values and nulls values and missing values etc. in this dataset we found the some values are missing in Cast and Directors columns so we decide to remove that colums also in release year there are sum values also missing we fill that values with mean and we also fill the country columns null values with mode in. After completion of 1st step in 2nd step we plot the data in form of graph, pie plot and bar plot and line plot and also box plot, and in box plot we find some outlier so we remove the outliers. And we also find some hidden information by plot the graph. In this dataset there are two types of contents 30.86% is Tv shows and 69.14% is movies includes. We have reached a conclusion from our analysis from the content added over years that Netflix is focusing movies and TV shows (From 2016 data we get to know that Movies is increased by 80% and TV shows is increased by 73% compare). From the dataset insights we can conclude that the most number of TV Shows released in 2017 and for Movies it is 2020. On Netflix USA has the largest number of contents. And most of the countries preferred to produce movies more than TV shows. And then India comers on number 2 hos produce  the largest movies and the tv show. Most of the movies are belonging to 3 categories. TOP 3 content categories are International movies , dramas , comedies. there are 35.1% movies and tv shows are from the United State.\n",
        "12.7% movies and tv shows are from the India.\n",
        "5.5% tv shows and movies from the United Kingdom.\n",
        "3.1% tv shows and movies are from the Japan.\n",
        "2.5% movies and shows are from the South Korea. In text analysis (NLP) I used stop words, removed punctuations , stemming & TF-IDF vectorizer and other functions of NLP. Applied different clustering models like K-means, hierarchical, Agglomerative clustering, DBSCAN on data we got the best cluster arrangements.By applying different clustering algorithms to our dataset .we get the optimal number of cluster is equal to 3\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElOyZTsoAde8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}